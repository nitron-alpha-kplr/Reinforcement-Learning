{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkkB9KGR4YL5C06nAxRJBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitron-alpha-kplr/Reinforcement-Learning/blob/main/Workshop_Frozen_Lake_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workshop : Frozen Lake - Solution"
      ],
      "metadata": {
        "id": "R7POgN37Fl_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q* Learning with FrozenLake 4x4\n",
        "In this Notebook, we'll implement an agent that plays FrozenLake.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The goal of this game is to go from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H). However, the ice is slippery, so you won't always move in the direction you intend (stochastic environment)\n",
        "\n",
        "Prerequisites ️\n",
        "Before diving on the notebook you need to understand:\n",
        "\n",
        "The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) Article\n",
        "Q-learning Article\n",
        "In the video version we implemented a Q-learning agent that learns to play OpenAI Taxi-v2 with Numpy."
      ],
      "metadata": {
        "id": "7qmC5RcwFtr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install gym==0.7.4."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXyD_hLJFvi_",
        "outputId": "af04d02c-de02-4bf6-9ce6-d174c8f317fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym==0.7.4.\n",
            "  Downloading gym-0.7.4.tar.gz (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.2/152.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym==0.7.4.) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from gym==0.7.4.) (2.27.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gym==0.7.4.) (1.16.0)\n",
            "Collecting pyglet>=1.2.0 (from gym==0.7.4.)\n",
            "  Downloading pyglet-2.0.7-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.0/841.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7.4.) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7.4.) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7.4.) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->gym==0.7.4.) (3.4)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.7.4-py3-none-any.whl size=204668 sha256=91ffa8778facb035057005a753f1c907338efb46eb6c8f12f847922cebc54efb\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/55/26/d9b9f975bc80a53913520dd7ded672660cf82a01ac3b0a8769\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed gym-0.7.4 pyglet-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import the required libraries.\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "#create the environment usign OpenAI Gym\n",
        "env = gym.make(\"FrozenLake-v0\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oSg1ANlF-K9",
        "outputId": "6424b924-0447-4f9c-8a12-fcd6f7c53ab6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake-v0\n",
            "[2023-06-22 08:04:44,436] Making new env: FrozenLake-v0\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:  Create the Q-table and initialize it"
      ],
      "metadata": {
        "id": "FiuzAgM8Iyg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "\n",
        "print(f\"Action Space : {action_size} | State Space: {state_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiYZzty-F_t9",
        "outputId": "d20e0a83-2278-4af8-ff9c-0868f8f8d3c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space : 4 | State Space: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxbhTT3fGSXD",
        "outputId": "affeeb87-e9fa-4438-92c6-08958daea968"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(16, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2:  Create the Q-table and initialize it"
      ],
      "metadata": {
        "id": "XWt5tUm7I9w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_episodes = 15000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability\n",
        "decay_rate = 0.005             # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "ACbq0iDTGev8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 : Q-Learning Algorithm"
      ],
      "metadata": {
        "id": "YJ4da29HJDEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of rewards\n",
        "rewards = []\n",
        "\n",
        "#until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        #Choose an action a in the current world state (s)\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "\n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * \\\n",
        "                                                        (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "\n",
        "        total_rewards += reward\n",
        "\n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "\n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True:\n",
        "            break\n",
        "\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "\n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhu_-qrKGhbl",
        "outputId": "3f9a080f-d4d7-4ccb-c104-da5286101159"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score over time: 0.48486666666666667\n",
            "[[2.78505243e-01 8.01246530e-02 3.53051296e-02 8.23637021e-02]\n",
            " [3.65954843e-02 4.81971049e-02 6.71212767e-03 1.74387491e-01]\n",
            " [4.10808519e-03 4.62863024e-02 3.52806713e-02 1.63100538e-01]\n",
            " [1.47182145e-04 2.45615459e-02 1.24188751e-02 7.50735114e-02]\n",
            " [3.00536755e-01 1.01330607e-02 2.81851501e-02 3.63119797e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.79109591e-04 2.15700028e-04 8.65869253e-03 8.96046995e-06]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.05883065e-03 6.65513988e-02 3.14000180e-02 5.48654176e-01]\n",
            " [4.14922794e-03 5.86043640e-01 1.96043163e-03 1.00408392e-02]\n",
            " [1.66319776e-01 7.32941217e-04 5.42433654e-03 6.17749811e-04]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [7.99296156e-02 1.13922996e-01 8.76559461e-01 9.42495679e-02]\n",
            " [2.03797095e-01 9.98960823e-01 1.48995506e-01 1.95286040e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "\n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = np.argmax(qtable[state,:])\n",
        "\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "\n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd-hVG8wIFCC",
        "outputId": "b2ce32ca-0029-45df-8a7c-38baef84ac94"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 16\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 25\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 21\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "Number of steps 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Système de récompense :\")\n",
        "for state in range(env.env.nS):\n",
        "    for action in range(env.env.nA):\n",
        "        transitions = env.env.P[state][action]\n",
        "        for prob, next_state, reward, _ in transitions:\n",
        "            print(f\"État {state}, Action {action}, État suivant {next_state} : {reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGtkq4xFIJmJ",
        "outputId": "e225bcd4-cd18-4f66-e8f0-ec1b950ef2e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Système de récompense :\n",
            "État 0, Action 0, État suivant 0 : 0.0\n",
            "État 0, Action 0, État suivant 0 : 0.0\n",
            "État 0, Action 0, État suivant 4 : 0.0\n",
            "État 0, Action 1, État suivant 0 : 0.0\n",
            "État 0, Action 1, État suivant 4 : 0.0\n",
            "État 0, Action 1, État suivant 1 : 0.0\n",
            "État 0, Action 2, État suivant 4 : 0.0\n",
            "État 0, Action 2, État suivant 1 : 0.0\n",
            "État 0, Action 2, État suivant 0 : 0.0\n",
            "État 0, Action 3, État suivant 1 : 0.0\n",
            "État 0, Action 3, État suivant 0 : 0.0\n",
            "État 0, Action 3, État suivant 0 : 0.0\n",
            "État 1, Action 0, État suivant 1 : 0.0\n",
            "État 1, Action 0, État suivant 0 : 0.0\n",
            "État 1, Action 0, État suivant 5 : 0.0\n",
            "État 1, Action 1, État suivant 0 : 0.0\n",
            "État 1, Action 1, État suivant 5 : 0.0\n",
            "État 1, Action 1, État suivant 2 : 0.0\n",
            "État 1, Action 2, État suivant 5 : 0.0\n",
            "État 1, Action 2, État suivant 2 : 0.0\n",
            "État 1, Action 2, État suivant 1 : 0.0\n",
            "État 1, Action 3, État suivant 2 : 0.0\n",
            "État 1, Action 3, État suivant 1 : 0.0\n",
            "État 1, Action 3, État suivant 0 : 0.0\n",
            "État 2, Action 0, État suivant 2 : 0.0\n",
            "État 2, Action 0, État suivant 1 : 0.0\n",
            "État 2, Action 0, État suivant 6 : 0.0\n",
            "État 2, Action 1, État suivant 1 : 0.0\n",
            "État 2, Action 1, État suivant 6 : 0.0\n",
            "État 2, Action 1, État suivant 3 : 0.0\n",
            "État 2, Action 2, État suivant 6 : 0.0\n",
            "État 2, Action 2, État suivant 3 : 0.0\n",
            "État 2, Action 2, État suivant 2 : 0.0\n",
            "État 2, Action 3, État suivant 3 : 0.0\n",
            "État 2, Action 3, État suivant 2 : 0.0\n",
            "État 2, Action 3, État suivant 1 : 0.0\n",
            "État 3, Action 0, État suivant 3 : 0.0\n",
            "État 3, Action 0, État suivant 2 : 0.0\n",
            "État 3, Action 0, État suivant 7 : 0.0\n",
            "État 3, Action 1, État suivant 2 : 0.0\n",
            "État 3, Action 1, État suivant 7 : 0.0\n",
            "État 3, Action 1, État suivant 3 : 0.0\n",
            "État 3, Action 2, État suivant 7 : 0.0\n",
            "État 3, Action 2, État suivant 3 : 0.0\n",
            "État 3, Action 2, État suivant 3 : 0.0\n",
            "État 3, Action 3, État suivant 3 : 0.0\n",
            "État 3, Action 3, État suivant 3 : 0.0\n",
            "État 3, Action 3, État suivant 2 : 0.0\n",
            "État 4, Action 0, État suivant 0 : 0.0\n",
            "État 4, Action 0, État suivant 4 : 0.0\n",
            "État 4, Action 0, État suivant 8 : 0.0\n",
            "État 4, Action 1, État suivant 4 : 0.0\n",
            "État 4, Action 1, État suivant 8 : 0.0\n",
            "État 4, Action 1, État suivant 5 : 0.0\n",
            "État 4, Action 2, État suivant 8 : 0.0\n",
            "État 4, Action 2, État suivant 5 : 0.0\n",
            "État 4, Action 2, État suivant 0 : 0.0\n",
            "État 4, Action 3, État suivant 5 : 0.0\n",
            "État 4, Action 3, État suivant 0 : 0.0\n",
            "État 4, Action 3, État suivant 4 : 0.0\n",
            "État 5, Action 0, État suivant 5 : 0\n",
            "État 5, Action 1, État suivant 5 : 0\n",
            "État 5, Action 2, État suivant 5 : 0\n",
            "État 5, Action 3, État suivant 5 : 0\n",
            "État 6, Action 0, État suivant 2 : 0.0\n",
            "État 6, Action 0, État suivant 5 : 0.0\n",
            "État 6, Action 0, État suivant 10 : 0.0\n",
            "État 6, Action 1, État suivant 5 : 0.0\n",
            "État 6, Action 1, État suivant 10 : 0.0\n",
            "État 6, Action 1, État suivant 7 : 0.0\n",
            "État 6, Action 2, État suivant 10 : 0.0\n",
            "État 6, Action 2, État suivant 7 : 0.0\n",
            "État 6, Action 2, État suivant 2 : 0.0\n",
            "État 6, Action 3, État suivant 7 : 0.0\n",
            "État 6, Action 3, État suivant 2 : 0.0\n",
            "État 6, Action 3, État suivant 5 : 0.0\n",
            "État 7, Action 0, État suivant 7 : 0\n",
            "État 7, Action 1, État suivant 7 : 0\n",
            "État 7, Action 2, État suivant 7 : 0\n",
            "État 7, Action 3, État suivant 7 : 0\n",
            "État 8, Action 0, État suivant 4 : 0.0\n",
            "État 8, Action 0, État suivant 8 : 0.0\n",
            "État 8, Action 0, État suivant 12 : 0.0\n",
            "État 8, Action 1, État suivant 8 : 0.0\n",
            "État 8, Action 1, État suivant 12 : 0.0\n",
            "État 8, Action 1, État suivant 9 : 0.0\n",
            "État 8, Action 2, État suivant 12 : 0.0\n",
            "État 8, Action 2, État suivant 9 : 0.0\n",
            "État 8, Action 2, État suivant 4 : 0.0\n",
            "État 8, Action 3, État suivant 9 : 0.0\n",
            "État 8, Action 3, État suivant 4 : 0.0\n",
            "État 8, Action 3, État suivant 8 : 0.0\n",
            "État 9, Action 0, État suivant 5 : 0.0\n",
            "État 9, Action 0, État suivant 8 : 0.0\n",
            "État 9, Action 0, État suivant 13 : 0.0\n",
            "État 9, Action 1, État suivant 8 : 0.0\n",
            "État 9, Action 1, État suivant 13 : 0.0\n",
            "État 9, Action 1, État suivant 10 : 0.0\n",
            "État 9, Action 2, État suivant 13 : 0.0\n",
            "État 9, Action 2, État suivant 10 : 0.0\n",
            "État 9, Action 2, État suivant 5 : 0.0\n",
            "État 9, Action 3, État suivant 10 : 0.0\n",
            "État 9, Action 3, État suivant 5 : 0.0\n",
            "État 9, Action 3, État suivant 8 : 0.0\n",
            "État 10, Action 0, État suivant 6 : 0.0\n",
            "État 10, Action 0, État suivant 9 : 0.0\n",
            "État 10, Action 0, État suivant 14 : 0.0\n",
            "État 10, Action 1, État suivant 9 : 0.0\n",
            "État 10, Action 1, État suivant 14 : 0.0\n",
            "État 10, Action 1, État suivant 11 : 0.0\n",
            "État 10, Action 2, État suivant 14 : 0.0\n",
            "État 10, Action 2, État suivant 11 : 0.0\n",
            "État 10, Action 2, État suivant 6 : 0.0\n",
            "État 10, Action 3, État suivant 11 : 0.0\n",
            "État 10, Action 3, État suivant 6 : 0.0\n",
            "État 10, Action 3, État suivant 9 : 0.0\n",
            "État 11, Action 0, État suivant 11 : 0\n",
            "État 11, Action 1, État suivant 11 : 0\n",
            "État 11, Action 2, État suivant 11 : 0\n",
            "État 11, Action 3, État suivant 11 : 0\n",
            "État 12, Action 0, État suivant 12 : 0\n",
            "État 12, Action 1, État suivant 12 : 0\n",
            "État 12, Action 2, État suivant 12 : 0\n",
            "État 12, Action 3, État suivant 12 : 0\n",
            "État 13, Action 0, État suivant 9 : 0.0\n",
            "État 13, Action 0, État suivant 12 : 0.0\n",
            "État 13, Action 0, État suivant 13 : 0.0\n",
            "État 13, Action 1, État suivant 12 : 0.0\n",
            "État 13, Action 1, État suivant 13 : 0.0\n",
            "État 13, Action 1, État suivant 14 : 0.0\n",
            "État 13, Action 2, État suivant 13 : 0.0\n",
            "État 13, Action 2, État suivant 14 : 0.0\n",
            "État 13, Action 2, État suivant 9 : 0.0\n",
            "État 13, Action 3, État suivant 14 : 0.0\n",
            "État 13, Action 3, État suivant 9 : 0.0\n",
            "État 13, Action 3, État suivant 12 : 0.0\n",
            "État 14, Action 0, État suivant 10 : 0.0\n",
            "État 14, Action 0, État suivant 13 : 0.0\n",
            "État 14, Action 0, État suivant 14 : 0.0\n",
            "État 14, Action 1, État suivant 13 : 0.0\n",
            "État 14, Action 1, État suivant 14 : 0.0\n",
            "État 14, Action 1, État suivant 15 : 1.0\n",
            "État 14, Action 2, État suivant 14 : 0.0\n",
            "État 14, Action 2, État suivant 15 : 1.0\n",
            "État 14, Action 2, État suivant 10 : 0.0\n",
            "État 14, Action 3, État suivant 15 : 1.0\n",
            "État 14, Action 3, État suivant 10 : 0.0\n",
            "État 14, Action 3, État suivant 13 : 0.0\n",
            "État 15, Action 0, État suivant 15 : 0\n",
            "État 15, Action 1, État suivant 15 : 0\n",
            "État 15, Action 2, État suivant 15 : 0\n",
            "État 15, Action 3, État suivant 15 : 0\n"
          ]
        }
      ]
    }
  ]
}